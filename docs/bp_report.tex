\documentclass[11pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{colortbl}
\usepackage{color}

\title{Report}
\author{Zeljko Kraljevic}

\begin{document}
\maketitle

\section{General}

\begin{align}
        C &= \sum_{t} \sum_{a} \sum_{b \in C_{a}^{(t)}} \left( \underbrace{log \sigma \left( x_{b}^{(t)} \cdot 
                y_{a}^{(t)} \right)}_{u_p}
                + \sum_{c} \underbrace{log \sigma \left( - x_{b}^{(t)} \cdot y_{c}^{(t)} \right)}_{u_n} \right)
\end{align}

Setup when training:
\begin{itemize}
	\item I normalize the time of the whole dataset to be between 0-10 for all datasets
	\item We don't have alterations currently, everything is trained all the time
\end{itemize} 


\section{Subsampling}
I still subsample frequent words using $P(w_i) = 1 - \sqrt{\frac{1}{f(w_i)}}$. I also subsample documents in the following way:
\begin{itemize}
	\item From a training set I always take a fixed number $N$ of $(a, b)$ pairs
	\item From every document I take a fixed number $M$ of $(a, b)$ pairs limiting the number of paris having the same target with $K$. 
	\item When choosing a pair from document the closer the words in it are the higher the chance of it being choosen.
	\item Depending on $N, M$ I calculate the probability of taking a document so that the whole dataset is always equaly present in the subsampled training set. 
\end{itemize}

\section{Clustering}

The basic formula used for clustering is:
\begin{equation}
  p(c \mid d) = \frac{p(d \mid c) p(c)}{p(d)}
\end{equation}
Given that $p(c)$ and $p(d)$ are the same for every cluster and document, we get:
\begin{equation}
  p(c \mid d) \propto p(d \mid c)
\end{equation}
Now we have the probability of a word given a cluster and we define the probability of a document given a cluster to be the product of all words in the document:
\begin{equation}
  p(d \mid c) = \prod_{w_i \in d}{p(c \mid w_i) f_c(t_i)}
\end{equation}
Which means:
\begin{equation}
	p(c \mid d) = \prod_{w_i \in d}{p(c \mid w_i) f_c(t_i)}
\end{equation}
We take a $log$ of the probability for convenience:
\begin{equation}
  log(p(c \mid d)) = \sum_{w_i \in d}{log(p(c \mid w_i) f_c(t_i))}
\end{equation}
Once this is calculated the document is clustered with:
\begin{equation}
  doc\_cluster = \arg\max_{c \in C} log(p(c \mid d))
\end{equation}

\section{Time Prediction}
Time prediction is similar to clustering except we don't use the time limiting function:
\begin{equation}
	p(t \mid d) = \prod_{w_i \in d}{p(c \mid w_i)}
\end{equation}
We define the log of this probability as: $ l_c = log(p(t \mid d)) $ \
Now we can predict the time as:
\begin{equation}
  predicted\_time = \frac{\sum_{c \in C}{e^{l_c}t_c}}{\sum_c{e^{l_c}}}
\end{equation}
We multiply this by $\frac{e^{-z}}{e^{-z}}$ where $z = max(l_c)$ and get WHY (I know why, but how to explain it normally):
\begin{equation}
  predicted\_time = \frac{\sum_{c \in C}{e^{l_c - z}t_c}}{\sum_c{e^{l_c - z}}}
\end{equation}

\section{Finished Tests}
Notes:
\begin{itemize}
  \item Cap
  \item tau=0
  \item regularization
\end{itemize}
Results are in /develop/results/

\begin{table}[h!]
	\begin{tabular}{|l|l|c|c|c|l|} 
      		\rowcolor[gray]{0.7}
		\hline
		Notes & Dataset & Iterations & clusters & Tau & Name \\ \hline
		Without reg or cap & NIPS & 500 & 300 & 1 & normal \\ \hline
		Without reg or cap & NIPS & 500 & 300 & 0 & normal\_tau \\ \hline
		Without reg, normalization & NIPS & 500 & 300 & 1 & normalization \\ \hline
	\end{tabular}
\end{table}
\section{Running Tests}
Tests that are currently running, approximately it takes one day for a test to finish.
\begin{table}[h!]
	\begin{tabular}{|l|l|c|c|c|l|} 
      		\rowcolor[gray]{0.7}

		\hline
		Notes & Dataset & Iterations & clusters & Tau & Folder \\ \hline
		tau=0.01 & NIPS & 500 & 300 & 0 & nips\_tau\_001 \\ \hline
		tau=1 & NIPS & 500 & 300 & 0 & nips\_tau\_1 \\ \hline
    tau=0.01 & Tweets & 500 & 500 & 0 & tweets\_tau\_small \\ \hline

	\end{tabular}
\end{table}

\section{TODO}

\begin{itemize}
	\item Try using aleterations, not so easy to implement
\end{itemize}

\end{document}
